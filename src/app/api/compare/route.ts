import { supabase } from "@/utils/supabase";
import { openai } from "@ai-sdk/openai";
import { generateObject, generateText } from "ai";
import { z } from "zod";

const EvaluationSchema = z.object({
  prompt: z.string(),
  responseBasePrompt: z.string(),
  evaluationBasePrompt: z.object({
    criteria_scores: z.object({
      accuracy: z.number(),
      correctness: z.number(),
      relevance: z.number(),
      completeness: z.number(),
      aesthetics: z.number(),
      tone: z.number(),
    }),
    score: z.number(),
  }),
  enhancedPrompt: z.string(),
  responseEnhancedPrompt: z.string(),
  evaluationEnhancedPrompt: z.object({
    criteria_scores: z.object({
      accuracy: z.number(),
      correctness: z.number(),
      relevance: z.number(),
      completeness: z.number(),
      aesthetics: z.number(),
      tone: z.number(),
    }),
    score: z.number(),
  }),
  feedback: z.string(),
});

export async function POST(req: Request) {
  /* const { messages } = await req.json(); */
  try {
    const data = await req.json();
    const {
      basePrompt,
      enhancedPrompt,
      enhancedPromptResponse,
      enhancedPromptUsage,
      options,
    } = data;

    if (!basePrompt || !enhancedPromptResponse) {
      return new Response(
        JSON.stringify({
          error: "Missing 'prompt' or 'response' in request body.",
        }),
        { status: 400 }
      );
    }

    const systemInstruction = `You are a helpful assistant. You listen carefully to instructions. You can answer questions and provide information on a wide range of topics. Your answers should be clear, concise, and well-structured using proper Markdown syntax.
    Guidelines:
    - Use **bold** for emphasis
    - Use *italics* for subtle emphasis
    - Format links like [example](https://www.example.com), have text change color to blue for links
    - Use # Headings and ## Subheadings where appropriate
    - Use bullet points and numbered lists for structure
    - Format code using \`\`inline code\`\` or code blocks with triple backticks

    Do not use any HTML.  Just Markdown.
    All responses must follow these formatting rules consistently.`;

    const result = await generateText({
      model: openai("gpt-4o-mini"),
      temperature: 0.5,
      topK: 40,
      prompt: `${basePrompt}`,
      system: systemInstruction,
    });

    const responseBasePrompt = result.text.trim();

    const systemInstructionsCompare = `
      You are an impartial expert AI evaluator whose task is to assess and compare two responses generated by an AI assistant, both responding to the same user prompt but under different system instruction contexts.
      Your goal is not to assume one version is better by design, but to critically evaluate each response on its own merits according to clear, structured criteria — and then compare them directly to determine which, if any, demonstrates superior quality.
      The purpose of this evaluation is to determine whether the enhanced prompt or system instruction improves response quality — but not to assume that it does. You must remain neutral and allow the evaluation criteria to determine which is better.`;

    const compareInstructionsPrompt = (
      A: string,
      B: string,
      promptA: string,
      promptB: string
    ) => `
      When comparing, **ignore the order**—treat Response A and Response B purely by their content and do not assume any advantage based on position.

      Prompt used for Response A:
      ${promptA}

      Response A:
      ${A}

      Prompt used for Response B:
      ${promptB}

      Response B:
      ${B}

      1. Evaluate each response independently on the following criteria:
        **Accuracy** - Is the information factually correct?
        **Correctness** - Does it logically answer the user's prompt or question?
        **Relevance** - Does it stay on topic and directly address the prompt?
        **Completeness** - Are all important aspects of the user's prompt addressed?
        **Aesthetics** - Is the language clear, well-formatted, and readable?
        **Tone** - Is the tone appropriate, respectful, and helpful?

        For each criterion, give a **score from 1 to 10** for *each* response, where:
        - 1 = Very poor
        - 5 = Acceptable
        - 10 = Excellent

      You must score only based on what is present in the response, without comparing it to the other response at this stage.

      2. Finally, provide a detailed **comparison feedback**. This should:
      - Summarize the strengths and weaknesses of Response A.
      - Summarize the strengths and weaknesses of Response B.
      - Clearly compare the two responses, stating which one is better overall and providing specific reasons why, referencing the criteria and content as needed.
        Consider:
          Which response better addresses the user prompt and provides most useful information.
          Whether the enhanced system instruction was followed meaningfully (if applicable).
          Whether either response exhibits critical flaws or standout strengths.

      3. Provide an overall **score** (1-100) for *each* response. This score should reflect the overall quality of the response and should not just be an average of the criteria scores. Instead, it should:
      - Heavily penalize failures in Accuracy, Correctness, or Relevance.
      - Reward clarity, helpful tone, and thoroughness.
     - Evaluate each answer's quality independent of length. Deduct points if an answer is unnecessarily verbose without adding factual or relevant content.”
      - Reward a response that contains more information as long as it is relevant and useful.
      - Reflect real-world usefulness and trustworthiness.
        If a response fails in a critical category (e.g., factual accuracy or addressing the core user prompt), its overall score should reflect that failure even if other areas are strong.
        Conversely, a response that is technically correct but ignores key tone/formatting/system constraints should be penalized on completeness and tone, even if its content is strong.
      - If you considerany response to be better than the other, give it atlast a 1 point higher score then the other response

      Generate your output to match the required structure exactly. Do not include explanations or extra commentary outside of the structured data.

      {
        "prompt": "${promptA}",
        "responseBasePrompt": "${A}",
        "evaluationBasePrompt": { 
          "criteria_scores": {
            "accuracy": <1-10>,
            "correctness": <1-10>,
            "relevance": <1-10>,
            "completeness": <1-10>,
            "aesthetics": <1-10>,
            "tone": <1-10>
          },
          "score": <1-100>
        },
        "enhancedPrompt": "${promptB}",
        "responseEnhancedPrompt": "${B}",
        "evaluationEnhancedPrompt": {
          "criteria_scores": {
            "accuracy": <1-10>,
            "correctness": <1-10>,
            "relevance": <1-10>,
            "completeness": <1-10>,
            "aesthetics": <1-10>,
            "tone": <1-10>
          },
          "score": <1-100>
        },
        "feedback": "<Detailed explanation comparing Response 1 and Response 2, concluding which is better and why.>"
      }
    `;

    const resultCompare = async (prompt: string) =>
      await generateObject({
        model: openai("gpt-4o"),
        temperature: 0.2,
        system: systemInstructionsCompare,
        prompt,
        schema: EvaluationSchema,
      });

    const promptAB = compareInstructionsPrompt(
      responseBasePrompt,
      enhancedPromptResponse,
      basePrompt,
      enhancedPrompt
    );

    const promptBA = compareInstructionsPrompt(
      enhancedPromptResponse,
      responseBasePrompt,
      enhancedPrompt,
      basePrompt
    );

    const [resultAB, resultBA] = await Promise.all([
      resultCompare(promptAB),
      resultCompare(promptBA),
    ]);

    if (!resultAB.object || !resultBA.object) {
      return new Response(
        JSON.stringify({ error: "One or both evaluations failed." }),
        {
          status: 500,
        }
      );
    }

    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    function averageScores(scoreA: any, scoreB: any) {
      const avgCriteria = Object.fromEntries(
        Object.keys(scoreA.criteria_scores).map((key) => [
          key,
          (scoreA.criteria_scores[key] + scoreB.criteria_scores[key]) / 2,
        ])
      );
      const avgScore = (scoreA.score + scoreB.score) / 2;
      return {
        criteria_scores: avgCriteria,
        score: avgScore,
      };
    }

    const parsedA = resultAB.object;
    const parsedB = resultBA.object;

    const averagedBaseEvaluation = averageScores(
      parsedA.evaluationBasePrompt,
      parsedB.evaluationEnhancedPrompt
    );
    const averagedEnhancedEvaluation = averageScores(
      parsedA.evaluationEnhancedPrompt,
      parsedB.evaluationBasePrompt
    );

    const combinedFeedback = `${parsedA.feedback}\n\n---\n\n${parsedB.feedback}`;

    try {
      const { error } = await supabase.from("comparisons").insert([
        {
          prompt: parsedA.prompt,
          response_base_prompt: parsedA.responseBasePrompt,
          evaluation_base_prompt: averagedBaseEvaluation,
          base_prompt_usage: result.usage,
          response_enhanced_prompt: parsedA.responseEnhancedPrompt,
          evaluation_enhanced_prompt: averagedEnhancedEvaluation,
          enhanced_prompt_usage: enhancedPromptUsage,
          feedback: combinedFeedback,
          options,
        },
      ]);
      if (error) {
        return new Response(JSON.stringify({ error: error.message }), {
          status: 500,
        });
      }
    } catch (error) {
      console.error("Error inserting into Supabase:", error);
    }

    const evaluation = {
      originalOrder: parsedA,
      swappedOrder: parsedB,
      averaged: {
        basePromptEvaluation: averagedBaseEvaluation,
        enhancedPromptEvaluation: averagedEnhancedEvaluation,
        feedback: combinedFeedback,
      },
    };

    console.log(evaluation);

    return new Response(JSON.stringify(evaluation), {
      status: 200,
      headers: { "Content-Type": "application/json" },
    });
  } catch (error) {
    console.error("Evaluation error:", error);
    return new Response(JSON.stringify({ error: "Internal server error" }), {
      status: 500,
    });
  }
}
